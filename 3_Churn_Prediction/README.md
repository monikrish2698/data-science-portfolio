
# Analysing raw data: How do I begin?

Hello peeps! Recently I started interning at an AgriTech startup as a Data Science Consultant and this is my first official Data Science industry role. Hurray! 
This internship is a part of my academic engagement at the Technology Leaders Program at Plaksha University.<br>
I have always wondered how would I begin analysing a raw dataset. I know how to check for null values, impute them (meaningfully!), rename columns to readability, etc. Then I ask the question of whether this is enough. Because now what's left is sklearn libraries doing their magic on my data. Then I ask again, is it this simple? Shouldn't the model building be 20% of my time and the rest 80% with data analysis? <br>
Then it was clear to me that the data science approach is not standardised because the data is not. Every data is unique in its own way and that's when the human enters the loop. Data analysts and scientists know what to look for and how to connect the dots to build the final data picture.
I started researching on few check-boxes that I can tick when working with raw data. I identified a few important steps that helped me streamline my work and also helped me stay on track during the analysis phase. Trust me it is very easy to get lost in data. I find something interesting and after 20 code cells in my Jupyter notebook, I am lost and mostly have to trace back to where I left off. So, I decided to write down what helped me with the hopes that someone else would find it helpful too :)<br>
Below are the following steps I took. I would also bring to the attention of readers that this is not exhaustive. It is my effort in ensuring that any beginner can keep track of their analysis and build a habit to further personalise the below set of steps after working on several projects. I have taken help from multiple sources in making it beginner friendly. As a guide, I have attached a churn prediction notebook to practice the below steps. <br>
Let's hop on!<br>
Before jumping on to analysing the data, familiarise yourself with the data at a surface level. Have some understanding of the background of the problem like how the data is collected, knowledge about the columns and values (like what does churn mean, is it the same everywhere, how does it usually affect the business), etc. This will really help tons when analysing the data and it provides a way for you to interpret the results in a meaningful way rather than saying the mean of group A is higher than group B. 

## 1. Data Completeness and Quality
Data is beautifully messy! The lower the mess, the better the analysis. But this is hardly the truth out there. In reality, the data is messy and as data scientists or analysts, we have to ensure that we are aware of the mess and its impact on our analyses. What kind of mess are we talking about? Null values, noisy data (two, three, 4, 5, 6), different patterns of strings (Chennai, Chn, Bangalore, Blr), etc. Give importance here because this stage determines the next set of stages. If we miss something, then it is a ride from the beginning. When we begin, we usually don't catch all the details but with enough practice, we tune our brains to look for patterns that can potentially be hidden in GBs of data.

## 2. Data distribution
Have an eye on the distribution of the data. Data distribution lays the foundation for the upcoming statistical analyses and model building. Unrestricted to the above two, observing the data distribution allows one to identify outliers and any flaws in the data. From a model-building perspective, having an idea about the data distribution allows one to use appropriate models. You can verify certain assumptions that must hold for some models like linear regression. Some commonly used data distribution plots are histograms and empirical cumulative distribution functions.

## 3. Data correlation and analyses
This part is where you start looking at the most relevant features that will be going to your first dummy model. Dummy models are usually the first model that you build with default or expert-suggested parameters, etc. As data scientists and analysts, one must know to identify these basic features. The most common would be to check the correlation of the features with respect to the predicting variable. Correlation is not causation, but always a good first step to having correlated features in the model. Other statistical tests can be used in identifying the relevant features. Mindful about multicollinearity where features are correlated amongst themselves which can make observing the final isolated effects of features on the predicting variable. I will be writing another blog on the most commonly used statistical test in hypothesis testing.

## 4. Feature Selection
The step on data correlation and statistical analyses finally filters down to this step. Choosing the relevant features based on the model building. Classical ML models work best when the best and the right features make it to the model. FOMO on features can lead to OVERFITTING. Background knowledge about the problem statement can help in cherry-picking relevant features. Since data scientist roles are cross-functional, sometimes a data scientist is expected to wear the subject expert hat with little to no background in the subject.

## 5. Class imbalances or longtailed distributions
A common encounter out in the real world - not enough representation. Uff! Either the data is dominated by one class or, a long-tailed distribution with many values having a lower count. This is a concerning problem and needs to be addressed before building the model. Dropping classes with lesser representation in the class can be dropped based on the count. If there are a couple of classes then they can be dropped for the dummy model. But this option is not highly recommended. SMOTE technique can be used in generating synthetic samples (either by down-sampling or up-sampling). This can engender noise in the data and be detrimental to the model if the generation affects the data distribution. This step is very specific to the problem statement and needs some experimentation before finalising one step.

After, the above steps one finally enters the model-building stage. The boring stage of all. This is hardly any piece of code (thanks to all the libraries out there). But the exploration stage after this is fascinating. You get an opportunity to explore the data with evidence and move in the right direction. 
I hope the above steps helped you in getting started with your first data science project. Personally to me, the above steps always help me get started with the next step after building the first dummy model. It gives me clarity on where to head next. And the process continues until a satisfactory result (accuracy, parsimonious, and interoperability). Follow me on Medium for more articles on the topic. Cheers!
